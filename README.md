# Avika - Your Supportive Chat Companion

A Streamlit-based chatbot application that provides mental wellness support by recommending mental health resources based on user input. It uses Qdrant for vector storage and Google Gemini for generative AI capabilities.

---

## ‚öôÔ∏è Setup and Configuration

### 1. Environment Variables

Create a `.env` file in the root project directory by copying the example below. The application will automatically load these variables on startup.

```dotenv
# .env

# Required API Key for Google Gemini
GEMINI_API_KEY=your_gemini_api_key_here

# Required: Qdrant Connection Details
QDRANT_URL=http://localhost:6333 # Or your Qdrant Cloud URL
# QDRANT_API_KEY=your_qdrant_cloud_api_key_if_applicable # Uncomment and set if using Qdrant Cloud with API key

# Optional: Path for Avika Titles (defaults to ./Avika_Titles.docx)
# AVIKA_TITLES_PATH=./Avika_Titles.docx

# Optional: Path to the folder containing DOCX documents for the knowledge base (defaults to ./Avika_Docs/)
# Used by scripts/populate_db.py
# AVIKA_DOCS_PATH=./Avika_Docs/

# Optional: Name for the Qdrant collection for document chunks (defaults to avika_doc_chunks)
# QDRANT_DOC_COLLECTION_NAME=avika_doc_chunks
```

*   `GEMINI_API_KEY`: **Required.** Your API key for Google Gemini.
*   `QDRANT_URL`: **Required.** The URL of your Qdrant instance (e.g., `http://localhost:6333` for a local Docker setup, or your Qdrant Cloud endpoint).
*   `QDRANT_API_KEY`: Optional. Your API key for Qdrant Cloud, if authentication is enabled.
*   `AVIKA_TITLES_PATH`: Optional. Path to the `Avika_Titles.docx` file. Defaults to `./Avika_Titles.docx`.
*   `AVIKA_DOCS_PATH`: Optional. Path to the folder for DOCX documents, used by `scripts/populate_db.py`. Defaults to `./Avika_Docs/`.
*   `QDRANT_DOC_COLLECTION_NAME`: Optional. Name of the Qdrant collection for document chunks. Defaults to `avika_doc_chunks`.

### 2. Qdrant Vector Database Setup

You need a running Qdrant instance for Avika to store and search document embeddings.

**Option A: Local Qdrant with Docker (Recommended for Development)**
   ```bash
   docker run -p 6333:6333 -p 6334:6334 \
       -v $(pwd)/qdrant_storage:/qdrant/storage:z \
       qdrant/qdrant
   ```
   This creates a `qdrant_storage` directory in your project root for persistent data. Ensure `QDRANT_URL` in `.env` is `http://localhost:6333`.

**Option B: Qdrant Cloud**
   Use [Qdrant Cloud](https://cloud.qdrant.io/). Set `QDRANT_URL` and `QDRANT_API_KEY` (if applicable) in your `.env` file.

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```
Ensure you have Python 3.8+ installed. Consider using a virtual environment.

### 4. Populate the Vector Database (Qdrant)
Ensure your Qdrant instance is running and configured in `.env`. Then, run:
```bash
python scripts/populate_db.py
```
This processes documents from `Avika_Docs/` and titles from `Avika_Titles.docx` into Qdrant.

## üöÄ How to Run the Streamlit Application

1.  Ensure your `.env` file is correctly configured (see Setup).
2.  Make sure your Qdrant instance is running and accessible.
3.  Install dependencies: `pip install -r requirements.txt`.
4.  Run the Streamlit app:
    ```bash
    streamlit run streamlit_app.py
    ```
    This will typically open the app in your web browser (e.g., at `http://localhost:8501`). The app directly uses the `AvikaChat` logic and connects to Qdrant as per your `.env` settings.

---

## üõ†Ô∏è Features

-   Session-aware conversations via Streamlit session state.
-   RAG-based mental health recommendations using Qdrant for document retrieval.
-   LLM-powered (Google Gemini) intent detection, empathetic responses, and resource recommendations.
-   Safety guardrails and crisis response guidance.
-   Resources sourced from `Avika_Titles.docx`.
-   Knowledge base context from DOCX files in `Avika_Docs/` (via Qdrant).
-   Interactive chat interface with Streamlit.

---

## Tech Stack

-   Streamlit (Web application framework)
-   Qdrant (Vector database)
-   Sentence Transformers (Embeddings generation)
-   Google Gemini API (LLM for chat and intent detection)
-   Hugging Face Transformers (RoBERTa model for offensive language detection)
-   `python-docx` (DOCX processing)
-   `langchain-text-splitters` (Text splitting for document chunking - used in `populate_db.py`)
-   Python-dotenv (Environment variable management)
-   Numpy, Torch

---

## File Structure

```
.
‚îú‚îÄ‚îÄ streamlit_app.py     # Main Streamlit application with AvikaChat logic
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ populate_db.py   # Script to populate Qdrant
‚îú‚îÄ‚îÄ Avika_Docs/          # Folder for DOCX source documents
‚îú‚îÄ‚îÄ Avika_Titles.docx    # Document with titles, categories, embedding text
‚îú‚îÄ‚îÄ qdrant_storage/      # (Generated by local Qdrant Docker)
‚îú‚îÄ‚îÄ requirements.txt     # Python dependencies
‚îú‚îÄ‚îÄ .env.example         # Example environment file
‚îú‚îÄ‚îÄ .gitignore           # Git exclusions
‚îî‚îÄ‚îÄ README.md            # This documentation

# Potentially:
# ‚îú‚îÄ‚îÄ Dockerfile           # If you decide to containerize the Streamlit app
```

---

## Deploying the Streamlit App

The easiest way to deploy a public-facing Streamlit app is using **Streamlit Community Cloud**:
1.  Push your project to a GitHub repository.
2.  Sign up/log in at [share.streamlit.io](https://share.streamlit.io/) with GitHub.
3.  Deploy your app, selecting the repository, branch, and `streamlit_app.py`.
4.  **Crucially, configure the necessary secrets (Environment Variables) in the Streamlit Cloud settings for your app** (e.g., `GEMINI_API_KEY`, `QDRANT_URL`, `QDRANT_API_KEY`).

For other deployment options (like Dockerizing the Streamlit app and hosting it on platforms like Render, Fly.io, or Google Cloud Run), you would create a `Dockerfile` for the Streamlit app and follow the platform's deployment guides.

---

## Next Steps & Potential Enhancements

*   **Refine Prompts:** Continuously test and refine the LLM prompts in `streamlit_app.py` (within `AvikaChat`) for better conversation quality and recommendation accuracy.
*   **Error Handling:** Enhance error handling and user feedback within the Streamlit app for edge cases or API issues.
*   **Advanced Monitoring:** If deployed for wider use, integrate more comprehensive logging and monitoring.
*   **CI/CD:** For regular updates, especially if using self-hosting, set up a CI/CD pipeline.
*   **Modularization:** While `AvikaChat` is now in `streamlit_app.py`, consider moving it back to `avika_chat.py` and importing it if `streamlit_app.py` becomes too large, to maintain better organization.
